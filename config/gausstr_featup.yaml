# GaussTR FeatUp Configuration for PyTorch Lightning
# Uses pre-extracted FeatUp features (512-dim)

# Hydra output configuration - consolidate all outputs under work_dirs
defaults:
  - _self_

hydra:
  run:
    dir: work_dirs/${experiment_name}/${run_name}_${now:%Y%m%d_%H%M%S}
  sweep:
    dir: work_dirs/${experiment_name}
    subdir: ${run_name}_${now:%Y%m%d_%H%M%S}
  output_subdir: .hydra  # Keep hydra config in a subdirectory

# Experiment settings
seed: 42
work_dir: work_dirs/${experiment_name}  # Base work directory
logger: "mlflow"  # Options: "mlflow", "tensorboard", "wandb"
experiment_name: "gausstr_featup"
run_name: null  # Optional: specific run name for MLflow (auto-generated if null)
mlflow_tracking_uri: "sqlite:///mlruns/mlflow.db"  # SQLite database
mlflow_artifact_location: "mlruns/artifacts"  # Artifact storage
use_rich_progress: true
early_stopping: false
test_after_training: true

# Model configuration
model:
  num_queries: 300
  embed_dims: 256
  feat_dims: 512

  # Neck config
  neck_in_channels: 512
  neck_out_channels: 256
  neck_norm_type: "LN2d"

  # Decoder config
  decoder_num_layers: 3
  decoder_embed_dims: 256
  decoder_num_heads: 8
  decoder_ffn_channels: 2048
  decoder_num_levels: 4

  # Head config
  head_reduce_dims: 128
  head_image_shape: [432, 768]
  head_patch_size: 16
  head_depth_limit: 51.2
  head_text_protos: "ckpts/text_proto_embeds_clip.pth"
  head_prompt_denoising: true
  head_num_segment_classes: 18

  # Voxelizer config
  vol_range: [-40, -40, -1, 40, 40, 5.4]
  voxel_size: 0.4

  # Training config (passed to LightningModule)
  # Note: values below are for batch_size=2. For bs=4: lr=0.0004, warmup_iters=100
  learning_rate: 0.0002  # bs=4: 0.0004 (linear scaling)
  weight_decay: 0.005
  warmup_iters: 200  # bs=4: 100 (halved when bs doubles)
  warmup_factor: 0.001
  lr_milestones: [16]
  lr_gamma: 0.1
  # steps_per_epoch is auto-calculated from dataloader size
  gradient_clip_val: 35.0

  # Data preprocessor
  mean: [123.675, 116.28, 103.53]
  std: [58.395, 57.12, 57.375]

# Data configuration
data:
  data_root: "/mnt/nvme0/data/nuscenes"
  train_ann_file: "/mnt/nvme0/data/nuscenes/nuscenes_infos_train.pkl"
  val_ann_file: "/mnt/nvme0/data/nuscenes/nuscenes_infos_val.pkl"
  input_size: [432, 768]
  resize_lim: [0.48, 0.48]
  depth_root: "/mnt/nvme0/data/nuscenes_unidepth"
  feats_root: "/mnt/nvme0/data/nuscenes_featup"
  sem_seg_root: "/mnt/nvme0/data/nuscenes_sam3"
  batch_size: 2  # bs=4: num_workers=8, lr=0.0004, warmup_iters=100
  num_workers: 4  # bs=4: 8
  pin_memory: true
  persistent_workers: true

# Trainer configuration
trainer:
  max_epochs: 24
  accelerator: "gpu"
  devices: 8
  strategy: "ddp"
  precision: "16-mixed"
  gradient_clip_val: 35.0
  gradient_clip_algorithm: "norm"
  accumulate_grad_batches: 1
  val_check_interval: 1.0
  check_val_every_n_epoch: 1
  log_every_n_steps: 50
  deterministic: false
  benchmark: true
  sync_batchnorm: false
  num_sanity_val_steps: 2

# Optional: checkpoint to resume from
# resume_from: null
# load_from: null
