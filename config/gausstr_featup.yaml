# GaussTR FeatUp Configuration for PyTorch Lightning
# Uses pre-extracted FeatUp features (512-dim)

# Hydra output configuration - consolidate all outputs under work_dirs
defaults:
  - _self_

hydra:
  run:
    dir: work_dirs/${experiment_name}/${run_name}_${now:%Y%m%d_%H%M%S}
  sweep:
    dir: work_dirs/${experiment_name}
    subdir: ${run_name}_${now:%Y%m%d_%H%M%S}
  output_subdir: .hydra  # Keep hydra config in a subdirectory

# Experiment settings
seed: 42
work_dir: work_dirs/${experiment_name}  # Base work directory
logger: "mlflow"  # Options: "mlflow", "tensorboard", "wandb"
experiment_name: "gausstr_featup"
run_name: null  # Optional: specific run name for MLflow (auto-generated if null)
mlflow_tracking_uri: "sqlite:///mlruns/mlflow.db"  # SQLite database
mlflow_artifact_location: "mlruns/artifacts"  # Artifact storage
use_rich_progress: true
early_stopping: false
test_after_training: true

# Model configuration
model:
  num_queries: 300
  embed_dims: 256
  feat_dims: 768

  # Neck config
  neck_in_channels: 768
  neck_out_channels: 256
  neck_norm_type: "LN2d"

  # Decoder config
  decoder_num_layers: 3
  decoder_embed_dims: 256
  decoder_num_heads: 8
  decoder_ffn_channels: 2048
  decoder_num_levels: 4

  # Head config
  head_reduce_dims: 128
  head_image_shape: [432, 768]
  head_patch_size: 16
  head_depth_limit: 51.2
  head_text_protos: "ckpts/text_proto_embeds_dinov3clip.pth"
  head_prompt_denoising: true
  head_num_segment_classes: 18

  # Voxelizer config (defaults match original GaussTR)
  vol_range: [-40, -40, -1, 40, 40, 5.4]
  voxel_size: 0.4
  filter_gaussians: false  # Set true to filter out-of-range and low-opacity Gaussians
  opacity_thresh: 0.0  # Minimum opacity for filtering (only used when filter_gaussians=true)
  sigma_factor: 3.0  # Gaussian radius multiplier for voxelization

  # Training config (passed to LightningModule)
  learning_rate: 0.0004  # Linear scaling: lr * (bs/2)
  weight_decay: 0.005
  warmup_iters: 100  # Halved when batch size doubles
  warmup_factor: 0.001
  lr_milestones: [16]
  lr_gamma: 0.1
  # steps_per_epoch is auto-calculated from dataloader size
  gradient_clip_val: 35.0

  # Data preprocessor
  mean: [123.675, 116.28, 103.53]
  std: [58.395, 57.12, 57.375]

  # Performance optimizations
  torch_compile: false  # Disabled - may conflict with custom CUDA ops
  compile_mode: "reduce-overhead"  # Options: "default", "reduce-overhead", "max-autotune"

# Data configuration
data:
  data_root: "data/nuscenes"
  train_ann_file: "data/nuscenes/nuscenes_infos_train.pkl"
  val_ann_file: "data/nuscenes/nuscenes_infos_val.pkl"
  input_size: [432, 768]
  resize_lim: [0.48, 0.48]
  depth_root: "data/nuscenes_unidepth"
  feats_root: "data/nuscenes_dinov3clip"
  sem_seg_root: null  # Set to "data/nuscenes_sam3" to enable segmentation
  batch_size: 4
  num_workers: 8
  pin_memory: true
  persistent_workers: true
  prefetch_factor: 3  # Prefetch batches per worker for faster data loading

# Trainer configuration
trainer:
  max_epochs: 24
  accelerator: "gpu"
  devices: 8
  strategy: "ddp_find_unused_parameters_true"
  precision: "16-mixed"
  gradient_clip_val: 35.0
  gradient_clip_algorithm: "norm"
  accumulate_grad_batches: 1
  val_check_interval: 1.0
  check_val_every_n_epoch: 1
  log_every_n_steps: 50
  deterministic: false
  benchmark: true
  sync_batchnorm: false
  num_sanity_val_steps: 2

# Optional: checkpoint to resume from
# resume_from: null
# load_from: null
